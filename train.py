
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""
A minimal training script for SiT using PyTorch DDP.
"""
import torch
# the first flag below was False when we tested this script but True makes A100 training a lot faster:
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torchvision.datasets import ImageFolder
from torchvision import transforms
import numpy as np
from collections import OrderedDict
from PIL import Image
from copy import deepcopy
from glob import glob
from time import time
import argparse
import logging
import os

from models import SiT_models
from download import find_model
from transport import create_transport, Sampler
# from diffusers.models import AutoencoderKL
from pretrained_VAE import load_pretrain_vqvae
from train_utils import parse_transport_args
import wandb_utils
from cifar10 import train_dataset, test_dataset

#################################################################################
#                             Training Helper Functions                         #
#################################################################################

@torch.no_grad()
def update_ema(ema_model, model, decay=0.9999):
    """
    Step the EMA model towards the current model.
    """
    ema_params = OrderedDict(ema_model.named_parameters())
    model_params = OrderedDict(model.named_parameters())

    for name, param in model_params.items():
        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed
        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)


def requires_grad(model, flag=True):
    """
    Set requires_grad flag for all parameters in a model.
    """
    for p in model.parameters():
        p.requires_grad = flag


def cleanup():
    """
    End DDP training.
    """
    dist.destroy_process_group()


def create_logger(logging_dir):
    """
    Create a logger that writes to a log file and stdout.
    """
    if dist.get_rank() == 0:  # real logger
        logging.basicConfig(
            level=logging.INFO,
            format='[\033[34m%(asctime)s\033[0m] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            handlers=[logging.StreamHandler(), logging.FileHandler(f"{logging_dir}/log.txt")]
        )
        logger = logging.getLogger(__name__)
    else:  # dummy logger (does nothing)
        logger = logging.getLogger(__name__)
        logger.addHandler(logging.NullHandler())
    return logger


def center_crop_arr(pil_image, image_size):
    """
    Center cropping implementation from ADM.
    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126
    """
    while min(*pil_image.size) >= 2 * image_size:
        pil_image = pil_image.resize(
            tuple(x // 2 for x in pil_image.size), resample=Image.BOX
        )

    scale = image_size / min(*pil_image.size)
    pil_image = pil_image.resize(
        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC
    )

    arr = np.array(pil_image)
    crop_y = (arr.shape[0] - image_size) // 2
    crop_x = (arr.shape[1] - image_size) // 2
    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])


def main(args):
    """
    Trains a new SiT model on a single GPU (no DDP).
    """
    assert torch.cuda.is_available(), "Training currently requires at least one GPU."

    # -------------------------
    # single-GPU setup (no DDP)
    # -------------------------
    device = torch.device("cuda:0")
    torch.cuda.set_device(device)
    seed = args.global_seed
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    print(f"Starting single-GPU run, seed={seed}.")

    # Setup an experiment folder (single-process)
    os.makedirs(args.results_dir, exist_ok=True)
    experiment_index = len(glob(f"{args.results_dir}/*"))
    model_string_name = args.model.replace("/", "-")  # e.g., SiT-XL/2 --> SiT-XL-2
    experiment_name = f"{experiment_index:03d}-{model_string_name}-" \
                      f"{args.path_type}-{args.prediction}-{args.loss_weight}"
    experiment_dir = f"{args.results_dir}/{experiment_name}"
    checkpoint_dir = f"{experiment_dir}/checkpoints"
    os.makedirs(checkpoint_dir, exist_ok=True)
    logger = create_logger(experiment_dir)
    logger.info(f"Experiment directory created at {experiment_dir}")

    entity = os.environ.get("ENTITY", None)
    project = os.environ.get("PROJECT", None)
    if args.wandb:
        # keep existing behavour; initialize with entity/project if available
        wandb_utils.initialize(args, entity, experiment_name, project)

    # -------------------------
    # Create model + EMA
    # -------------------------
    assert args.image_size % 8 == 0, "Image size must be divisible by 8 (for the VAE encoder)."
    latent_size = args.image_size // 8
    model = SiT_models[args.model](
        input_size=latent_size,
        num_classes=args.num_classes
    )
    ema = deepcopy(model).to(device)

    # Move model to device (no DDP wrapper)
    model = model.to(device)

    # -------------------------
    # Optimizer (created before possible ckpt load so opt.load_state_dict works)
    # -------------------------
    opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0)

    # If checkpoint provided, load model/ema/opt/args
    if args.ckpt is not None:
        ckpt_path = args.ckpt
        state_dict = find_model(ckpt_path)
        model.load_state_dict(state_dict["model"])
        ema.load_state_dict(state_dict["ema"])
        # only load optimizer if shapes match / state present
        if "opt" in state_dict and state_dict["opt"] is not None:
            try:
                opt.load_state_dict(state_dict["opt"])
            except Exception as e:
                logger.warning(f"Could not load optimizer state: {e}")
        # restore args if checkpoint saved them
        if "args" in state_dict:
            args = state_dict["args"]

    requires_grad(ema, False)

    transport = create_transport(
        args.path_type,
        args.prediction,
        args.loss_weight,
        args.train_eps,
        args.sample_eps
    )  # default: velocity
    transport_sampler = Sampler(transport)

    # vae = AutoencoderKL.from_pretrained(f"stabilityai/sd-vae-ft-{args.vae}").to(device)
    vae = load_pretrain_vqvae()
    logger.info(f"SiT Parameters: {sum(p.numel() for p in model.parameters()):,}")

    # -------------------------
    # Setup data (single GPU)
    # -------------------------
    local_batch_size = int(args.global_batch_size)  # for single GPU, local == global
    dataset = train_dataset

    # If you want deterministic shuffling: use torch.Generator with seed
    # shuffle=True is fine for normal training
    loader = DataLoader(
        dataset,
        batch_size=local_batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=True
    )
    logger.info(f"Dataset contains {len(dataset):,} images ({getattr(args, 'data_path', 'N/A')})")

    # Prepare models for training:
    update_ema(ema, model, decay=0)  # initialize EMA with synced weights
    model.train()  # enable embedding dropout for classifier-free guidance
    ema.eval()  # EMA model should always be in eval mode

    # Variables for monitoring/logging purposes:
    train_steps = 0
    log_steps = 0
    running_loss = 0.0
    start_time = time()

    # Labels to condition the model with (feel free to change):
    use_cfg = args.cfg_scale > 1.0

    logger.info(f"Training for {args.epochs} epochs...")
    for epoch in range(args.epochs):
        logger.info(f"Beginning epoch {epoch}...")
        for x, y in loader:
            x = x.to(device)
            y = y.to(device)
            with torch.no_grad():
                # Map input images to latent space + normalize latents:
                # x = vae.encode(x).latent_dist.sample().mul_(0.18215)
                x = vae.encode(x)
            model_kwargs = dict(y=y, return_act=args.disp)
            loss_dict = transport.training_losses(model, x, model_kwargs)
            loss = loss_dict["loss"].mean()
            opt.zero_grad()
            loss.backward()
            opt.step()
            update_ema(ema, model)

            # Log loss values:
            running_loss += loss.item()
            log_steps += 1
            train_steps += 1

            if train_steps % args.log_every == 0:
                torch.cuda.synchronize()
                end_time = time()
                steps_per_sec = log_steps / (end_time - start_time)
                avg_loss = running_loss / log_steps
                logger.info(f"(step={train_steps:07d}) Train Loss: {avg_loss:.4f}, Train Steps/Sec: {steps_per_sec:.2f}")
                if args.wandb:
                    wandb_utils.log(
                        {"train loss": avg_loss, "train steps/sec": steps_per_sec},
                        step=train_steps
                    )
                running_loss = 0.0
                log_steps = 0
                start_time = time()

            # Save SiT checkpoint:
            if train_steps % args.ckpt_every == 0 and train_steps > 0:
                checkpoint = {
                    "model": model.state_dict(),
                    "ema": ema.state_dict(),
                    "opt": opt.state_dict(),
                    "args": args
                }
                checkpoint_path = f"{checkpoint_dir}/{train_steps:07d}.pt"
                torch.save(checkpoint, checkpoint_path)
                logger.info(f"Saved checkpoint to {checkpoint_path}")

            # Sampling (single GPU)
            if train_steps % args.sample_every == 0 and train_steps > 0:
                logger.info("Generating EMA samples...")
                # prepare labels / zs for sampling
                n = local_batch_size
                ys = torch.randint(args.num_classes, size=(n,), device=device)
                zs = torch.randn(n, 4, latent_size, latent_size, device=device)

                if use_cfg:
                    zs = torch.cat([zs, zs], 0)
                    y_null = torch.tensor([args.num_classes] * n, device=device)
                    ys = torch.cat([ys, y_null], 0)
                    sample_model_kwargs = dict(y=ys, cfg_scale=args.cfg_scale)
                    model_fn = ema.forward_with_cfg
                else:
                    sample_model_kwargs = dict(y=ys)
                    model_fn = ema.forward

                sample_fn = transport_sampler.sample_ode()  # default to ode sampling
                samples = sample_fn(zs, model_fn, **sample_model_kwargs)[-1]

                if use_cfg:
                    samples, _ = samples.chunk(2, dim=0)

                # decode and postprocess
                samples = vae.decode(samples * 1.2963932).sample  # keep your scaling
                # On single GPU, out_samples are just the decoded samples
                out_samples = samples.detach()

                if args.wandb:
                    wandb_utils.log_image(out_samples, train_steps)
                logger.info("Generating EMA samples done.")

    model.eval()  # disable randomized embedding dropout for final eval
    logger.info("Done!")
    cleanup()



if __name__ == "__main__":
    # Default args here will train SiT-XL/2 with the hyperparameters we used in our paper (except training iters).
    parser = argparse.ArgumentParser()
    parser.add_argument("--data-path", type=str, required=True)
    parser.add_argument("--results-dir", type=str, default="results")
    parser.add_argument("--model", type=str, choices=list(SiT_models.keys()), default='SiT-XS/1')
    parser.add_argument("--image-size", type=int, choices=[256, 512], default=256)
    parser.add_argument("--num-classes", type=int, default=10)
    parser.add_argument("--epochs", type=int, default=80)
    parser.add_argument("--global-batch-size", type=int, default=32)
    parser.add_argument("--global-seed", type=int, default=0)
    parser.add_argument("--vae", type=str, choices=["ema", "mse"], default="ema")  # Choice doesn't affect training
    parser.add_argument("--num-workers", type=int, default=4)
    parser.add_argument("--log-every", type=int, default=100)
    parser.add_argument("--ckpt-every", type=int, default=5000)
    parser.add_argument("--sample-every", type=int, default=5000)
    parser.add_argument("--cfg-scale", type=float, default=4.0)
    parser.add_argument("--wandb", action="store_true")
    parser.add_argument("--ckpt", type=str, default=None,
                        help="Optional path to a custom SiT checkpoint")
    parser.add_argument("--disp", action="store_true",
                        help="Toggle to enable Dispersive Loss")

    parse_transport_args(parser)
    args = parser.parse_args()
    main(args)

